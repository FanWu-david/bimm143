---
title: "Class08"
author: "Fan Wu(PID:A15127541)"
format: pdf
toc: true
---
Data was downloaded from the class website as a CSV file.

## Data Import
```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df)
```

The first column `diagnosis` is the expert opinion on the sample(i.e. patient FNA). 

```{r}
head(wisc.df$diagnosis)
```
Remove the diagnosis from data for subsequent analysis
```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
dim(wisc.data)
```

Store the diagnosis as a vector for use later when we compare our results to those from experts in the field
```{r}
# Create diagnosis vector for later 
diagnosis <- factor(wisc.df$diagnosis)
```
> Q1. How many observations are in this dataset?

There are `r nrow(wisc.data)` observations/patients in the dataset

> Q2. How many of the observations have a malignant diagnosis?

There are 212 malignant diagnosis
```{r}
#finds out how many Benign/ Malignant there are in the sample
table(diagnosis)
```
> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
variables <- colnames(wisc.data)

grep("_mean", variables)

results <- grep("_mean", variables, value = T)

results

total_use_of_mean <- length(results)
total_use_of_mean
```

## Principal Component Analysis (PCA)
In general we want to scale and center our data prior to PCA, 
to ensure that each features contribute equally to the analysis

The `prcomp()` function to do PCA has a `scale = FALSE` default.

We almost always want to set `scale = True` in `prcomp()`, so that certain columns/variables with large standard deviation and mean won't impact when ccompared to others just because the units of measurement are on different scales.

```{r}
# Check column means and standard deviations
wisc_mean <- colMeans(wisc.data)

wisc_sd <- apply(wisc.data,2,sd)

max(wisc_mean)

min(wisc_mean)

max(wisc_sd)

min(wisc_sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE )
# Look at summary of results
summary(wisc.pr)
```

The main PC result figure is called a "score plot" or "PC plot" or "ordination plot"...

```{r}
library(ggplot2)

# wisc.pr$x

ggplot(wisc.pr$x)+ aes(x = PC1, y = PC2, col = diagnosis) + geom_point()
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

```{r}
summary(wisc.pr)
```

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs

Create a biplot of the wisc.pr using the biplot() function.
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

Very packed; It's hard to understand, because there are so many dimensions involved. 

```{r}
biplot(wisc.pr)
```


```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```


> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?


```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,c(1,3)], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

## Variance Explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
tot_variance = sum(pr.var)
# Variance explained by each principal component: pve
pve <- pr.var / tot_variance

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

# Communicate PCA Results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
PC1_loading <- wisc.pr$rotation[,1]
PC1_loading["concave.points_mean"]
```

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

5Pcs

```{r}
summary(wisc.pr)
```

## Hierarchial Clustering

The goal of this section is to do hierarchical clustering of the original data. Recall from class that this type of clustering does not assume in advance the number of natural groups that exist in the data.

As part of the preparation for hierarchical clustering, the distance between all pairs of observations are computed. Furthermore, there are different ways to link clusters together, with single, complete, and average being the most common linkage methods.

First scale the `wisc.data` data and assign the result to `data.scaled`.

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to `data.dist`.

```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage. Manually specify the method argument to `hclust()` and assign the results to `wisc.hclust`.

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h = 19, col="red", lty=2)
```

## Selecting number of clusters

In this section, you will compare the outputs from your hierarchical clustering model to the actual diagnoses. Normally when performing unsupervised learning like this, a target variable (i.e. known answer or labels) isn’t available. We do have it with this dataset, however, so it can be used to check the performance of the clustering model.

When performing supervised learning - that is, when you’re trying to predict some target variable of interest and that target variable is available in the original data - using clustering to create new features may or may not improve the performance of the final model.

This exercise will help you determine if, in this case, hierarchical clustering provides a promising new feature.

Use cutree() to cut the tree so that it has 4 clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

maybe 6


## Combining methods (PCA & Clustering)
- Most important

Clustering the orginial data was not very productive. The PCA results look promising. Here we combine these methods by clustering from our PCA results. 
In other words, "clustering in PC space"..

```{r}
# Take the first 3 PCs 
dist.pc <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(dist.pc, method = "ward.D2")
```

View the tree..
```{r}
plot(wisc.pr.hclust)
abline(h=70, col = "red")
```

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```


To get our clustering membership vector (i.e. our main clustering result) we "cut the tree at a desired height or to yield a desired number of "k" groups.

```{r}
grps <- cutree(wisc.pr.hclust, k = 2)
table(grps)
```

How does this clustering grps compare to the expert diagnosis

```{r}
# Compare to actual diagnoses
table(grps, diagnosis)
```

Sensitivity : TP/ (TP +FN)
Specificity: TN / (TN + FP)

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

K-means clustering is skipped as an optional section

```{r}
# table(___, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```


## 7 Prediction

We can use our PCA model for prediction with new input patient samples

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
head(npc)
```

```{r}
g <- as.factor(grps)

plot(wisc.pr$x[,1:2], col = g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

Group 1

```{r}
#g <- as.factor(grps)

#plot(wisc.pr$x[,1:2], col = g)

table(grps, diagnosis)
```

